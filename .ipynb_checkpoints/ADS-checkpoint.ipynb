{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads_helper import c,get_incorrectly_tagged_indices,interpret_data\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Goal</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>I care most about ...</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4827f377eb734b4e9f8cf1b2d0b1624c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(options={'All toxic comments being flagged': 0, 'Non-toxic comments not being flagged as toxic'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Recipe</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c8c7cf76ab49c2847d3fb9ee26daf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='col', options=('Mentions', 'Toxic Comments', 'Non-Toxic Comments',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Sample Comment</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131ec4712e9d4a8399ee1d4859beceac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='nationality', options=('white', 'black', 'female', 'homosexual_gay…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Ingredients</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be769e981dc24e559a86048b4fa62151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='col', options=('What percentage of the toxic comments are classifi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Tagged Comment</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a3fd869b084d3a85d68c0941705483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='nationality', options=('white', 'black', 'female', 'homosexual_gay…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Evaluation</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6337bcdfe5b44757bae97252a7771efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual, FloatSlider\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from IPython.core.display import HTML\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "tdfn =pd.read_csv(\"known_identity_df.csv\",index_col=0)\n",
    "sdf = pd.read_csv(\"summary_df.csv\",index_col=0)\n",
    "\n",
    "#using tdfn and sdf from above\n",
    "classes_of_interest = ['white','black','female','homosexual_gay_or_lesbian','christian','hindu']\n",
    "options={\"All toxic comments being flagged\":0,\\\n",
    "         \"Non-toxic comments not being flagged as toxic\":1,\\\n",
    "         \"Percentage of comments that are flagged as toxic are not\":2}\n",
    "inv_options = {v:k for k,v in options.items()}\n",
    "\n",
    "\n",
    "\n",
    "def display_df_widget(display_df,col):\n",
    "    display(display_df.sort_values(col,ascending=False))\n",
    "\n",
    "def sorted_df_widget(display_df,cols):\n",
    "    return interactive(display_df_widget,display_df = fixed(display_df.loc[classes_of_interest]),col=display_df.columns)\n",
    "\n",
    "\n",
    "def get_sample_text(base_df,nationality,toxicity):\n",
    "    toxicity = (toxicity == \"Toxic\")*1\n",
    "    sub_df = base_df[(base_df[nationality] > .5) & (base_df[\"toxicity\"] == toxicity)]\n",
    "    print(sub_df.sample(1)[\"comment_text\"].values[0])\n",
    "\n",
    "    \n",
    "def get_explained_text(base_df,nationality,mention_threshold,toxicity,c):\n",
    "    toxicity = (toxicity == \"Toxic\")*1\n",
    "    indices = get_incorrectly_tagged_indices(base_df,nationality,mention_threshold,toxicity)\n",
    "    #indices = get_incorrect_examples(tdfn,'christian',mention_threshold,0)\n",
    "\n",
    "    exp = interpret_data(c.predict_proba, base_df.comment_text,base_df[\"bin_toxicity\"],['non_toxic','toxic'],indices,1,True)\n",
    "\n",
    "\n",
    "def get_recipe_widget(base_df):\n",
    "    recipe_df = base_df.iloc[:,0:4]\n",
    "    recipe_df.columns = [\"Mentions\",\"Toxic Comments\",\"Non-Toxic Comments\",\"Not Mentioned\"]\n",
    "    return sorted_df_widget(recipe_df,recipe_df.columns)\n",
    "\n",
    "def get_ingredient_widget(base_df):\n",
    "    idf = base_df.iloc[:,8:11]\n",
    "    idf.columns = [\"What percentage of the toxic comments are classified as toxic? Low values means many toxic comments were not flagged\",\\\n",
    "                   \"What percent of comments that are not toxic are flagged as toxic? High values mean many non-toxic comments are flagged as toxic\",\\\n",
    "                  \"What percentage of comments that are flagged as toxic are not? High values indicate rate of incorrect toxic labeling\"]\n",
    "#     idf.columns =[\"(# Comments Predicted Toxic)/(# Toxic Comments)\",\\\n",
    "#                  \"(# Comments Incorrectly Predicted Toxic)/(# Comments Predicted Toxic)\"]\n",
    "    idf = idf.apply(lambda x: round(x,2))\n",
    "    return sorted_df_widget(idf,idf.columns)\n",
    "\n",
    "def select_importance_widget():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def get_sample_text_widget(base_df):\n",
    "    return interactive(get_sample_text,base_df=fixed(base_df),\\\n",
    "                       nationality = classes_of_interest,\\\n",
    "                       toxicity=[\"Toxic\",\"Non-Toxic\"])\n",
    "\n",
    "\n",
    "def get_explained_text_widget(base_df):\n",
    "    return interactive(get_explained_text,base_df = fixed(base_df),\\\n",
    "                                          nationality=classes_of_interest,\\\n",
    "                                          mention_threshold=fixed(.1),\n",
    "                                          toxicity=[\"Toxic\",\"Non-Toxic\"],\\\n",
    "                                          c=fixed(c))\n",
    "                              \n",
    "\n",
    "def get_fpr_risk_widget(base_df):\n",
    "    idf = base_df.iloc[:,8:11]\n",
    "    high_f_rate = idf.iloc[:,-1]\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def get_question_widget():\n",
    "    slider = widgets.SelectMultiple(options = options)\n",
    "    return slider\n",
    "\n",
    "\n",
    "def get_registered_output(widget,func,*func_args,**kw_func_args):\n",
    "    output = widgets.Output()\n",
    "    def update_output(change_dict):\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            func(change_dict,*func_args,**kw_func_args)\n",
    "    \n",
    "    widget.observe(update_output)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def goal_func(change_dict,base_df):\n",
    "    option = change_dict['old']['index'][0]\n",
    "    idf = base_df.iloc[:,8:11].loc[classes_of_interest]\n",
    "    idf = idf.sort_values(by=[idf.columns[option]],ascending = False)\n",
    "    idf = idf.iloc[:,[option]]*100\n",
    "    idf.columns = [inv_options[option]]\n",
    "    display(HTML(f\"<h3>You chose <span style='color:blue'>'{inv_options[option]}'</span> as your goal: </h3>\"))\n",
    "    display(HTML(\"<p>Accordingly, your groups of interest rank as follows:</p>\"))\n",
    "    if option == 0:\n",
    "        s = (idf\\\n",
    "         .style\\\n",
    "         .format({idf.columns[0]:'{0:,.0f}%'})\\\n",
    "         .bar(color=['#d65f5f', '#5fba7d'],vmin=0,vmax=100,subset = [idf.columns[0]])\\\n",
    "         .set_caption('Results'))\n",
    "    else:\n",
    "        s = (idf\\\n",
    "         .style\\\n",
    "         .format({idf.columns[0]:'{0:,.0f}%'})\\\n",
    "             .bar(subset = [idf.columns[0]], color=['#5fba7d','#d65f5f'],vmin=0,vmax=100)\\\n",
    "         .set_caption('Results'))\n",
    "    display(s)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "importance_widget = get_question_widget()\n",
    "goal_output = get_registered_output(importance_widget,goal_func,sdf)\n",
    "recipe_widget = get_recipe_widget(sdf)\n",
    "ingredient_widget = get_ingredient_widget(sdf)\n",
    "text_widget = get_sample_text_widget(tdfn)\n",
    "exp_text_widget = get_explained_text_widget(tdfn)\n",
    "display(HTML(\"<h1>Goal</h1>\"))\n",
    "display(HTML(\"<h3>I care most about ...</h3>\"))\n",
    "display(importance_widget)\n",
    "display(HTML(\"<h1>Recipe</h1>\"))\n",
    "display(recipe_widget)\n",
    "display(HTML(\"<h1>Sample Comment</h1>\"))\n",
    "display(text_widget)\n",
    "display(HTML(\"<h1>Ingredients</h1>\"))\n",
    "display(ingredient_widget)\n",
    "display(HTML(\"<h1>Tagged Comment</h1>\"))\n",
    "display(exp_text_widget)\n",
    "display(HTML(\"<h1>Evaluation</h1>\"))\n",
    "display(goal_output)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
