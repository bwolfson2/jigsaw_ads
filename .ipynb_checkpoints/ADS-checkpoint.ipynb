{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benwolfson/anaconda3/envs/data_resp/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from ads_helper import c,get_incorrectly_tagged_indices,interpret_data\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Goal</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>I care most about ...</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01d1b6b4f9048c09114a5adbeba6529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(layout=Layout(width='max-content'), options={'All toxic comments being flagged': 0, 'Non-toxic …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Recipe</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3689013f7643c2a464fb22ffc4b0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='col', options=('Mentions', 'Toxic Comments', 'Non-Toxic Comments',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Sample Comment</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4195d9a5f3245b69ca5d1889bda1f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='nationality', options=('male', 'female', 'transgender', 'other_gen…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Ingredients</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f86a77fd0fd4dd9807305f95feeb6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='col', options=('What percentage of the toxic comments are classifi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Tagged Comment</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78926898aafa4fab97dc62b485a0e230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='nationality', options=('male', 'female', 'transgender', 'other_gen…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Evaluation</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a39df64ff984e79905dbfaf92ddc74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual, FloatSlider\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from IPython.core.display import HTML\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tdfn =pd.read_csv(\"known_identity_df.csv\",index_col=0)\n",
    "sdf = pd.read_csv(\"summary_df.csv\",index_col=0)\n",
    "\n",
    "#using tdfn and sdf from above\n",
    "\n",
    "options={\"All toxic comments being flagged\":0,\\\n",
    "         \"Non-toxic comments not being flagged as toxic\":1,\\\n",
    "         \"Percentage of comments that are flagged as toxic that are not toxic\":2}\n",
    "inv_options = {v:k for k,v in options.items()}\n",
    "\n",
    "classes_of_interest = tdfn.columns[-27:-3]#['male','white','black','female','homosexual_gay_or_lesbian','christian','hindu']\n",
    "\n",
    "def display_df_widget(display_df,col):\n",
    "    display(display_df.sort_values(col,ascending=False))\n",
    "\n",
    "def sorted_df_widget(display_df,cols):\n",
    "    return interactive(display_df_widget,display_df = fixed(display_df.loc[classes_of_interest]),col=display_df.columns)\n",
    "\n",
    "\n",
    "def get_sample_text(base_df,nationality,toxicity):\n",
    "    toxicity = (toxicity == \"Toxic\")*1\n",
    "    sub_df = base_df[(base_df[nationality] > .5) & (base_df[\"toxicity\"] == toxicity)]\n",
    "    print(sub_df.sample(1)[\"comment_text\"].values[0])\n",
    "\n",
    "    \n",
    "def get_explained_text(base_df,nationality,mention_threshold,toxicity,c):\n",
    "    toxicity = (toxicity == \"Toxic\")*1\n",
    "    indices = get_incorrectly_tagged_indices(base_df,nationality,mention_threshold,toxicity)\n",
    "    #indices = get_incorrect_examples(tdfn,'christian',mention_threshold,0)\n",
    "    np.random.seed(np.random.randint(0,10000))\n",
    "    np.random.shuffle(indices)\n",
    "    exp = interpret_data(c.predict_proba, base_df.comment_text,base_df[\"bin_toxicity\"],['non_toxic','toxic'],\\\n",
    "                         indices,1,True)\n",
    "\n",
    "\n",
    "def get_recipe_widget(base_df):\n",
    "    recipe_df = base_df.iloc[:,0:4]\n",
    "    recipe_df.columns = [\"Mentions\",\"Toxic Comments\",\"Non-Toxic Comments\",\"Not Mentioned\"]\n",
    "    return sorted_df_widget(recipe_df,recipe_df.columns)\n",
    "\n",
    "def get_ingredient_widget(base_df):\n",
    "    idf = base_df.iloc[:,8:11]\n",
    "    idf.columns = [\"What percentage of the toxic comments are classified as toxic? Low values means many toxic comments were not flagged\",\\\n",
    "                   \"What percent of comments that are not toxic are flagged as toxic? High values mean many non-toxic comments are flagged as toxic\",\\\n",
    "                  \"What percentage of comments that are flagged as toxic are not? High values indicate rate of incorrect toxic labeling\"]\n",
    "#     idf.columns =[\"(# Comments Predicted Toxic)/(# Toxic Comments)\",\\\n",
    "#                  \"(# Comments Incorrectly Predicted Toxic)/(# Comments Predicted Toxic)\"]\n",
    "    idf = idf.apply(lambda x: round(x,2))\n",
    "    return sorted_df_widget(idf,idf.columns)\n",
    "\n",
    "\n",
    "def get_sample_text_widget(base_df):\n",
    "    return interactive(get_sample_text,base_df=fixed(base_df),\\\n",
    "                       nationality = classes_of_interest,\\\n",
    "                       toxicity=[\"Toxic\",\"Non-Toxic\"])\n",
    "\n",
    "\n",
    "def get_explained_text_widget(base_df):\n",
    "    return interactive(get_explained_text,base_df = fixed(base_df),\\\n",
    "                                          nationality=classes_of_interest,\\\n",
    "                                          mention_threshold=fixed(.1),\n",
    "                                          toxicity=[\"Toxic\",\"Non-Toxic\"],\\\n",
    "                                          c=fixed(c))\n",
    "                              \n",
    "\n",
    "def get_fpr_risk_widget(base_df):\n",
    "    idf = base_df.iloc[:,8:11]\n",
    "    high_f_rate = idf.iloc[:,-1]\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def get_question_widget():\n",
    "    slider = widgets.SelectMultiple(options = options,layout={'width': 'max-content'})\n",
    "    return slider\n",
    "\n",
    "\n",
    "def get_registered_output(widget,func,*func_args,**kw_func_args):\n",
    "    output = widgets.Output()\n",
    "    def update_output(change_dict):\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            func(change_dict,*func_args,**kw_func_args)\n",
    "    \n",
    "    widget.observe(update_output)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def goal_func(change_dict,base_df):\n",
    "    option = change_dict['old']['index'][0]\n",
    "    idf = base_df.iloc[:,8:11].loc[classes_of_interest]\n",
    "    idf = idf.sort_values(by=[idf.columns[option]],ascending = False)\n",
    "    idf = idf.iloc[:,[option]]\n",
    "    if option == 0:\n",
    "        idf = idf.max()/idf\n",
    "    else:\n",
    "        idf = idf/idf.min()\n",
    "    idf.columns = [inv_options[option]]\n",
    "    display(HTML(f\"<h3>You chose <span style='color:blue'>'{inv_options[option]}'</span> as your goal: </h3>\"))\n",
    "    display(HTML(\"<p>Accordingly, your groups of interest rank as follows:</p>\"))\n",
    "    if option == 0:\n",
    "        s = (idf\\\n",
    "         .style\\\n",
    "         .format({idf.columns[0]:'{0:,.2f}'})\\\n",
    "         .bar(color=['#d65f5f', '#5fba7d'],vmin=0,vmax=3,subset = [idf.columns[0]])\\\n",
    "         .set_caption('Results'))\n",
    "    else:\n",
    "        s = (idf\\\n",
    "         .style\\\n",
    "         .format({idf.columns[0]:'{0:,.2f}'})\\\n",
    "             .bar(subset = [idf.columns[0]], color=['#5fba7d','#d65f5f'],vmin=0,vmax=3)\\\n",
    "         .set_caption('Results'))\n",
    "    display(s)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "importance_widget = get_question_widget()\n",
    "goal_output = get_registered_output(importance_widget,goal_func,sdf)\n",
    "recipe_widget = get_recipe_widget(sdf)\n",
    "ingredient_widget = get_ingredient_widget(sdf)\n",
    "text_widget = get_sample_text_widget(tdfn)\n",
    "exp_text_widget = get_explained_text_widget(tdfn)\n",
    "display(HTML(\"<h1>Goal</h1>\"))\n",
    "display(HTML(\"<h3>I care most about ...</h3>\"))\n",
    "display(importance_widget)\n",
    "display(HTML(\"<h1>Recipe</h1>\"))\n",
    "display(recipe_widget)\n",
    "display(HTML(\"<h1>Sample Comment</h1>\"))\n",
    "display(text_widget)\n",
    "display(HTML(\"<h1>Ingredients</h1>\"))\n",
    "display(ingredient_widget)\n",
    "display(HTML(\"<h1>Tagged Comment</h1>\"))\n",
    "display(exp_text_widget)\n",
    "display(HTML(\"<h1>Evaluation</h1>\"))\n",
    "display(goal_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( (tdfn.predicted_toxicity == 0) & (tdfn.clf_pred == 0) & (tdfn.bin_toxicity == 1) &(tdfn['white'] > 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['male', 'female', 'transgender', 'other_gender', 'heterosexual',\n",
       "       'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation',\n",
       "       'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist',\n",
       "       'other_religion', 'black', 'white', 'asian', 'latino',\n",
       "       'other_race_or_ethnicity', 'physical_disability',\n",
       "       'intellectual_or_learning_disability', 'psychiatric_or_mental_illness',\n",
       "       'other_disability'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdfn.columns[-27:-3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
